I am not able to understand what to do when we have to show variable overriding behaviour. The problem is if the variable does not exist anywhere it will err out everything.
If the variable is overriden by another file, that variable 


I want to connect with dipesh sir on skype. Is it okay?

role2_task_vars: 'role2_task2_vars_applied'

hosts vars are still not considered important

I still did not tell what overrides what



Role1 tasks include role2, role2 defaults only some are overridden by role1, others are as it is of role2.
Playbook


They are simply different scopes that exist independently of each other, no sharing of variable.

inventory host_vars

play vars_prompt 
play vars_files

block vars

role (and include_role) params

include params

host facts / cached set_facts 4

playbook group_vars

inventory group_vars

inventory file or script host vars 2

playbook host_vars/* 3

done
include_role


not done
set_facts / registered vars




role defaults (defined in role/defaults/main.yml) 1 overrided playbook group_vars/all 3 which shall not be true according to the documentation.

var_for_block and simpler way to debug variables instead of writing the whole line itself for debug print.
# Reference: https://stackoverflow.com/a/43341317


16:30 07 Apr 2022

Start learning about

Ansible config file

Jenkins
	pipeline
	freestyle job(controller of bash script)
	multi-branch pipeline
	Groovy
	types of pipeline

Nginx

Stages in pipeline are actually like individual freestyle jobs. Each freestyle job triggers another freestyle downstream job. Freestyle jobs do a single work. Freestyle jobs are much older, when pipeline did not came into existence. It's the chain of freestyle jobs that used to accomplish the same thing.  Another difference is pipelines automatically resume when controller is again available, while freestyle jobs stop immediately when controller is killed.

# Reference: https://www.youtube.com/watch?v=Ei_Nk14vruE

Why to use Declarative Pipeline instead of Scripted ones? It is more
convinient most of the times. It contains options-block which gives
code more space to write and reduces the need for deep nesting. It
also contains stage state caching and able to store environment
variables from earlier stages, which reduces the need to re-run all
the previous stages. It is shown with each stage marked separately
and easy to work with on BlueOcean UI. It allows the capability to
skip a stage and yet get informed for which stage is being skipped
and you can see its reason quit easily, than what scripted pipeline
provides. You can pretest part of code before agent is allocated to
reduce the need for checking out source code at each node, when it's
simply not required. Declarative pipeline uses plugins to ease declaring work.


I still don't know how to create multiple steps and stages in case of scripted pipelines.

git is a plugin and only declarative pipelines allows the use of plugins. As scripted pipelines are written in groovy, they don't use plugins.



# Reference: https://www.youtube.com/watch?v=B_2FXWI6CWg
A multibranch job is a folder of pipeline jobs. After scanning(which can be push bashed-webhook or poll bashed) an automatic build of all matching git-branches (using regex match or wildcards) happen.


Docker
# Reference: https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#dockerfile-instructions

RUN apt-get update && apt-get install -y \
    aufs-tools \
    ruby1.9.1-dev \
    s3cmd=1.1.* \
 && rm -rf /var/lib/apt/lists/*

In addition, when you clean up the apt cache by removing /var/lib/apt/lists it reduces the image size, since the apt cache is not stored in a layer. Since the RUN statement starts with apt-get update, the package cache is always refreshed prior to apt-get install.

Official Debian and Ubuntu images automatically run apt-get clean, so explicit invocation is not required.

Using pipes
-----------
RUN wget -O - https://some.site | wc -l > /number

If you want the command to fail due to an error at any stage in the pipe, prepend set -o pipefail && to ensure that an unexpected error prevents the build from inadvertently succeeding. For example:

RUN set -o pipefail && wget -O - https://some.site | wc -l > /number

The CMD instruction should be used to run the software contained in your image.
CMD ["executable", "param1", "param2"…]

docker run -it python, you’ll get dropped into a usable shell,


I have some problems in installing both apache, php with mysql. However there are LAMP images directly available which does so, and they have gone through the trouble for important reasons and OS issues, that I don't need to get unnecessarily into.